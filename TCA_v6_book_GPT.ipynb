{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d13d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:656: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:656: SyntaxWarning: invalid escape sequence '\\I'\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19772\\2247571458.py:656: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  process_docx_file(\"D:\\IPS assignments\\Assignment 5\\ŸÇÿ±ÿ¢ŸÜ ÿ≠⁄©€åŸÖ ÿßÿ±ÿ™ŸÇÿß€å ÿπŸÑ€å ÿ®ŸÜÿØ⁄Ø€å.docx\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned DOCX: cleaned2.docx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cleaned_docx': 'cleaned2.docx'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Robust OCR-cleaner \n",
    "- Conservative heuristics + audit CSV for every automatic correction.\n",
    "- Jupyter-friendly: call the functions directly.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import docx\n",
    "from docx.shared import Pt, Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from collections import Counter\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "from wordfreq import zipf_frequency\n",
    "class SafeDict:\n",
    "    def __init__(self, threshold=2.5, custom_words=None):\n",
    "        self.threshold = threshold\n",
    "        self.custom_words = set(custom_words or [])\n",
    "\n",
    "    def check(self, word):\n",
    "        if not word:\n",
    "            return False\n",
    "\n",
    "        w = word.lower().strip(\".,;:()[]{}\")\n",
    "\n",
    "        if w in self.custom_words:\n",
    "            return True\n",
    "\n",
    "        return zipf_frequency(w, \"en\") >= self.threshold\n",
    "\n",
    "    def suggest(self, word):\n",
    "        # Optional stub to avoid crashes\n",
    "        return []\n",
    "\n",
    "\n",
    "AUDIT_HEADER = [\"file\",\"para_index\",\"action\",\"pattern\",\"before_preview\",\"after_preview\"]\n",
    "\n",
    "# Utilities\n",
    "EN_DICT = SafeDict(\n",
    "    threshold=2.5,\n",
    "    custom_words={\n",
    "        \"econometrics\",\n",
    "        \"macroeconomics\",\n",
    "        \"keynesian\",\n",
    "        \"neoclassical\",\n",
    "    }\n",
    ")\n",
    "\n",
    "EN_WORD_RE = re.compile(r\"^[A-Za-z][A-Za-z']+$\")\n",
    "UR_WORD_RE = re.compile(r\"^[\\u0600-\\u06FF\\u0750-\\u077F]+$\")\n",
    "\n",
    "def is_valid_urdu(word):\n",
    "    return zipf_frequency(word, \"ur\") >= 2.0\n",
    "def is_word(w, threshold=2.5):\n",
    "    return zipf_frequency(w.lower(), \"en\") >= threshold\n",
    "\n",
    "URDU_SCRIPT_RE = re.compile(r'^[\\u0600-\\u06FF\\u0750-\\u077F]+$')\n",
    "\n",
    "def is_urdu_word(token: str) -> bool:\n",
    "    return bool(URDU_SCRIPT_RE.match(token))\n",
    "\n",
    "\n",
    "def _safe_preview(s, n=140):\n",
    "    return (s[:n] + \"...\") if len(s) > n else s\n",
    "UNICODE_HYPHEN_PATTERN = re.compile(\n",
    "    \"[\" +\n",
    "    \"\\u2010\"  # hyphen\n",
    "    \"\\u2011\"  # non-breaking hyphen\n",
    "    \"\\u2012\"  # figure dash\n",
    "    \"\\u2013\"  # en dash\n",
    "    \"\\u2014\"  # em dash\n",
    "    \"\\u2015\"  # horizontal bar\n",
    "    \"\\u00AD\"  # soft hyphen\n",
    "    \"]\"\n",
    ")\n",
    "\n",
    "def highlight_misspellings_in_paragraph(paragraph):\n",
    "    \"\"\"\n",
    "    Highlight misspelled English and Urdu words in a paragraph.\n",
    "    Visual-only modification of DOCX runs.\n",
    "    \"\"\"\n",
    "    text = paragraph.text\n",
    "\n",
    "    # Extract both English and Urdu tokens\n",
    "    words = re.findall(r\"\\b[A-Za-z\\u0600-\\u06FF\\u0750-\\u077F']+\\b\", text)\n",
    "\n",
    "    misspelled = set()\n",
    "\n",
    "    for w in words:\n",
    "        # Skip short tokens\n",
    "        if len(w) < 4:\n",
    "            continue\n",
    "\n",
    "        # Skip acronyms\n",
    "        if w.isupper():\n",
    "            continue\n",
    "\n",
    "        # English word\n",
    "        if EN_WORD_RE.match(w):\n",
    "            if not is_word(w):\n",
    "                misspelled.add(w)\n",
    "            continue\n",
    "\n",
    "        # Urdu word\n",
    "        if UR_WORD_RE.match(w):\n",
    "            if not is_valid_urdu(w):\n",
    "                misspelled.add(w)\n",
    "            continue\n",
    "\n",
    "        # Mixed / garbage tokens ‚Üí ignore safely\n",
    "\n",
    "    if not misspelled:\n",
    "        return\n",
    "\n",
    "    # Clear paragraph and rebuild runs\n",
    "    paragraph._p.clear_content()\n",
    "\n",
    "    tokens = re.split(r\"(\\b[A-Za-z\\u0600-\\u06FF\\u0750-\\u077F']+\\b)\", text)\n",
    "    for tok in tokens:\n",
    "        run = paragraph.add_run(tok)\n",
    "        if tok in misspelled:\n",
    "            run.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "\n",
    "def normalize_unicode_and_spaces(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # Replace non-breaking spaces\n",
    "    text = text.replace(\"\\u00A0\", \" \").replace(\"\\u2007\", \" \").replace(\"\\u202F\", \" \")\n",
    "    # Remove zero-width spaces\n",
    "    text = re.sub(r\"[\\u200B\\u200C\\u200D\\uFEFF]\", \"\", text)\n",
    "    # Normalize line breaks\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # Preserve double newlines (paragraph separation)\n",
    "    text = re.sub(r\"[ \\t\\f\\v]+\", \" \", text)  # collapse spaces/tabs\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\",text)        # collapse 3+ newlines to 2\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_em_dashes(text):\n",
    "    \"\"\"\n",
    "    Normalize OCR dash artifacts into proper em dashes.\n",
    "    Handles:\n",
    "    - word - word\n",
    "    - word--word\n",
    "    - word-- word\n",
    "    - word --word\n",
    "    Avoids:\n",
    "    - hyphenated words\n",
    "    - numeric ranges\n",
    "    - minus signs\n",
    "    \"\"\"\n",
    "\n",
    "    # 1Ô∏è‚É£ Normalize double or more hyphens between letters ‚Üí em dash\n",
    "    text = re.sub(\n",
    "        r'(?<=[A-Za-z])-{2,}(?=[A-Za-z])',\n",
    "        '‚Äî',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # 2Ô∏è‚É£ Normalize spaced dash between words ‚Üí em dash\n",
    "    text = re.sub(\n",
    "        r'(?<=[A-Za-z])\\s+[-‚Äì‚Äî]\\s+(?=[A-Za-z])',\n",
    "        ' ‚Äî ',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # 3Ô∏è‚É£ Normalize mixed spacing: word-- word / word --word\n",
    "    text = re.sub(\n",
    "        r'(?<=[A-Za-z])\\s*-{2,}\\s*(?=[A-Za-z])',\n",
    "        '‚Äî',\n",
    "        text\n",
    "    )\n",
    "\n",
    "    return text\n",
    "\n",
    "# Common repetitive OCR headers (specific to this book or similar structures)\n",
    "HEADER_PATTERNS = []\n",
    "\n",
    "def detect_repetitive_headers(paragraphs, min_repeats=3):\n",
    "    \"\"\"\n",
    "    Detect recurring short lines likely to be running headers or footers.\n",
    "    Returns a set of repeated short lines.\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    for p in paragraphs:\n",
    "        line = p.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Skip standalone page numbers or numbering artifacts\n",
    "        if re.match(r\"^[-‚Äì‚Äî]?\\s*\\(?\\d{1,3}\\)?\\s*[-‚Äì‚Äî]?$\", line):\n",
    "            continue\n",
    "\n",
    "        # Require at least 3 letters (skip roman numerals or very short tokens)\n",
    "        if len(re.findall(r\"[A-Za-z]\", line)) <= 2:\n",
    "            continue\n",
    "        normalized.append(line)\n",
    "\n",
    "    counts = Counter(normalized)\n",
    "\n",
    "    repetitive = {\n",
    "        line for line, freq in counts.items()\n",
    "        if freq >= min_repeats and not line.islower() and not line[0].isdigit()\n",
    "    }\n",
    "\n",
    "    return repetitive\n",
    "\n",
    "def remove_known_headers(paragraphs, detected_headers, audit_writer=None, filename=None):\n",
    "    \"\"\"\n",
    "    Removes repetitive headers detected by detect_repetitive_headers().\n",
    "    Keeps the FIRST occurrence of each header line, removes the rest.\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    seen_headers = set()\n",
    "\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        stripped = para.strip()\n",
    "\n",
    "        if stripped in detected_headers:\n",
    "            if stripped not in seen_headers:\n",
    "                # Keep first occurrence\n",
    "                seen_headers.add(stripped)\n",
    "                filtered.append(para)\n",
    "            else:\n",
    "                # Remove subsequent repetitions\n",
    "                if audit_writer:\n",
    "                    audit_writer.writerow([\n",
    "                        filename or \"\",\n",
    "                        i,\n",
    "                        \"remove_header\",\n",
    "                        \"repetitive_header_duplicate\",\n",
    "                        _safe_preview(stripped),\n",
    "                        \"\"\n",
    "                    ])\n",
    "            continue\n",
    "\n",
    "        filtered.append(para)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def fix_ocr_hyphenated_words(text):\n",
    "    \"\"\"\n",
    "    Fix OCR-broken words:\n",
    "    - Merge words broken with hyphen + space.\n",
    "    - Remove hyphen if the merged word is a valid English word.\n",
    "    - Keep hyphen if it forms a legitimate hyphenated word.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'([A-Za-z]+)-\\s+([A-Za-z]+)')\n",
    "\n",
    "    def merge_match(m):\n",
    "        first, second = m.group(1), m.group(2)\n",
    "        combined = first + second\n",
    "        if is_word(combined.lower()):   # valid English word ‚Üí remove hyphen\n",
    "            return combined\n",
    "        else:                           # not valid ‚Üí keep hyphen\n",
    "            return first + \"-\" + second\n",
    "\n",
    "    for _ in range(3):  # repeat for multi-stage breaks\n",
    "        text = pattern.sub(merge_match, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def enhanced_ocr_preclean(text):\n",
    "    \"\"\"Enhanced version of OCR cleaning specifically for academic texts like the Contemporary Economic Challenges\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # 1. Initial normalization\n",
    "    text = normalize_unicode_and_spaces(text)\n",
    "\n",
    "    # 2. Fix hyphenation and line breaks more aggressively\n",
    "    text = fix_ocr_hyphenated_words(text)\n",
    "\n",
    "    # 2. Normalize spaced hyphens\n",
    "    text = re.sub(r'\\s*-\\s*', \"-\", text)\n",
    "    \n",
    "    # 3. Clean academic-specific markers\n",
    "    text = re.sub(r'(?m)^\\s*\\[?[ivxIVX]+\\]?\\s*$','',text)  # Remove Roman numerals like [i], [ii]\n",
    "    text = re.sub(r\"(?m)^\\*\\s?.*\\n?\", \"\", text) # Remove any paragraph starting with an asterisk (*)\n",
    "    text = re.sub(r'\\s*\\[\\d{1,3}\\]\\s*', ' ', text)  # Remove [123]\n",
    "    ##text = re.sub(r'\\s?\\(\\s*[\\d\\s:;\\-‚Äì‚Äî,]+\\s*\\)', '', text) # Remove (1, 2-3:4;5)\n",
    "\n",
    "    # 4. Paragraph segmentation\n",
    "    paras = text.split('\\n\\n')\n",
    "    cleaned_paragraphs = []\n",
    "\n",
    "    for i, para in enumerate(paras):\n",
    "        text = para.strip()     \n",
    "\n",
    "        # Skip empty lines\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Skip empty or page-number-only lines\n",
    "        if text.isdigit() or re.match(r'^[0-9]{1,3}$', text.strip()):\n",
    "            continue\n",
    "\n",
    "        # Skip small lines that are likely just page numbers like \"- 3 -\" or \"(4)\"\n",
    "        if re.match(r'^[-‚Äì‚Äî]?\\s*\\(?\\d{1,3}\\)?\\s*[-‚Äì‚Äî]?$', text.strip()):\n",
    "            continue\n",
    "\n",
    "        cleaned_paragraphs.append(text)\n",
    "\n",
    "\n",
    "    # 5. Join cleaned paragraphs back\n",
    "    text = '\\n\\n'.join(cleaned_paragraphs)\n",
    "\n",
    "    # Protect time formats like 11.40 A.M or 9.15 P.M\n",
    "    text = re.sub(r'\\b(\\d{1,2})\\.(\\d{2})\\s*(A\\.M|P\\.M)\\b',r'\\1:\\2 \\3',text,flags=re.IGNORECASE)\n",
    "    \n",
    "    # 6. Remove numeric/footnote artifacts\n",
    "    # Remove lines that START with digits followed by optional spaces and a letter,\n",
    "    # then anything until end of line. Typically removes \"12 Chapter ‚Ä¶\", \"3A Stuff‚Ä¶\"\n",
    "    text = re.sub(r'^\\d+\\s*[A-Za-z].*?(?=\\n|$)', '', text, flags=re.MULTILINE)\n",
    "    # Remove 'Page 12' only if it's at the start of a line or alone\n",
    "    text = re.sub(r'^(?:Page\\s*\\d+)\\s*$', '', text, flags=re.MULTILINE)\n",
    "    # Remove 1‚Äì2 digit numbers that appear DIRECTLY after a word starting with a letter:\n",
    "    # Example: \"word23\" ‚Üí \"word\"\n",
    "    text = re.sub(r'(?<![A-Z])(?<=[a-z])\\d+', '', text)\n",
    "    # Remove lines starting with 1-2 digits followed by a word (no punctuation before)\n",
    "    text = re.sub(r'^\\s*\\d{1,2}\\s+[A-Za-z]+\\b.*$', '', text, flags=re.MULTILINE)\n",
    "    # Remove trailing blocks of lines that start with numbers ‚Äî usually end-of-book index blocks.\n",
    "    text = re.sub(r'(\\n\\s*\\d{1,2}\\s+.*)+\\Z', '', text)\n",
    "    # Remove (pg. 12) style footnotes\n",
    "    text = re.sub(r'\\(\\s*pg\\.?\\s*\\d+\\s*\\)', '', text, flags=re.IGNORECASE)\n",
    "    # Replace dot followed by alphanumeric (\".12\", \".a\", \".X12\") with a plain dot.\n",
    "    text = re.sub(r'(?<!\\d)\\.[0-9A-Za-z]+(?=\\s|$)', '.', text)\n",
    "    # Replace \".*\" ‚Üí \".\"\n",
    "    text = re.sub(r'\\.(\\*)', '.', text)\n",
    "    # Replace \". 12 Something‚Ä¶\" ‚Üí \". Something\".\n",
    "    text = re.sub(r'\\.\\s*\\d+\\s+(?=[A-Z])', '. ', text)\n",
    "    # Remove numbers appearing right after an opening quotation mark (e.g., \"23 ‚Üí \")\n",
    "    text = re.sub(r'\"\\s*(\\d+)(?=\\s|[A-Z])', '\"', text)\n",
    "    # Remove numbers immediately after a comma if followed by space and text or newline\n",
    "    text = re.sub(r'(?<!\\d),(\\d{1,3})(?=\\s+[A-Za-z\\u0600-\\u06FF])',',',text)\n",
    "    # Remove numbers after a full stop followed by space if followed by capital letter or end of paragraph\n",
    "    text = re.sub(r'\\.\\s+\\d+(?=\\s+[A-Z\\u0600-\\u06FF]|$)', '. ', text)\n",
    "\n",
    "    # 7. Special character handling\n",
    "    text = re.sub(r'[\"\"\"]', '\"', text)\n",
    "    text = re.sub(r'[\\u2018\\u2019]', \"'\", text) # standardise quotation marks\n",
    "\n",
    "    # 8. Final normalization\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    text = re.sub(r'(?<=[.!?])\\s{2,}(?=\\S)', ' ', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def merge_soft_split_paragraphs_text(text):\n",
    "    \"\"\"\n",
    "    Merge soft-split paragraphs in a text string:\n",
    "    - Current paragraph ends with lowercase letter and no punctuation\n",
    "    - Next paragraph starts with lowercase letter\n",
    "    \"\"\"\n",
    "    paras = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    merged_paras = []\n",
    "    i = 0\n",
    "    while i < len(paras):\n",
    "        para = paras[i]\n",
    "\n",
    "        # Merge soft split paragraphs\n",
    "        while (\n",
    "            i + 1 < len(paras) and\n",
    "            para[-1].islower() and                  # current ends lowercase\n",
    "            not para[-1] in \".!?\" and               # current does not end with punctuation\n",
    "            paras[i + 1].strip()[0].islower()       # next starts lowercase\n",
    "        ):\n",
    "            next_para = paras[i + 1].strip()\n",
    "            para = para + \" \" + next_para\n",
    "            i += 1  # skip merged paragraph\n",
    "\n",
    "        merged_paras.append(re.sub(r\"\\s+\", \" \", para).strip())\n",
    "        i += 1\n",
    "\n",
    "    return \"\\n\\n\".join(merged_paras)\n",
    "\n",
    "def merge_paragraphs_split_by_hyphen(text):\n",
    "    \"\"\"\n",
    "    Merge paragraphs that are split by a line containing only a hyphen:\n",
    "    - Keeps the hyphen in the merged sentence\n",
    "    - Leaves other paragraphs untouched\n",
    "    \"\"\"\n",
    "    paras = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    merged_paras = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(paras):\n",
    "        para = paras[i]\n",
    "\n",
    "        # Check if the next paragraph is a single hyphen\n",
    "        while i + 1 < len(paras) and paras[i + 1].strip() == \"-\":\n",
    "            next_para = paras[i + 2].strip() if i + 2 < len(paras) else \"\"\n",
    "            # Merge current paragraph + hyphen + next paragraph\n",
    "            para = f\"{para} - {next_para}\" if next_para else f\"{para} -\"\n",
    "            i += 2  # skip the hyphen paragraph and the next merged paragraph\n",
    "\n",
    "        merged_paras.append(re.sub(r\"\\s+\", \" \", para).strip())\n",
    "        i += 1\n",
    "\n",
    "    return \"\\n\\n\".join(merged_paras)\n",
    "\n",
    "def merge_broken_words(paragraphs, audit_writer=None, filename=None):\n",
    "    \"\"\"\n",
    "    Merge paragraphs split by broken hyphenated words,\n",
    "    skipping numeric/page-number/junk paragraphs in between.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(paragraphs):\n",
    "        cur = paragraphs[i].rstrip()\n",
    "        # Check for hyphen at end of paragraph\n",
    "        while True:\n",
    "            m = re.search(r'([A-Za-z]+)-\\s*$', cur)\n",
    "            if not m:\n",
    "                break\n",
    "\n",
    "            # Look ahead for the next valid paragraph to merge\n",
    "            j = i + 1\n",
    "            next_para = \"\"\n",
    "            while j < len(paragraphs):\n",
    "                candidate = paragraphs[j].strip()\n",
    "                # Skip junk paragraphs: page numbers, numeric, or single brackets\n",
    "                if re.match(r'^(\\[\\d+\\]|\\d+|[-‚Äì‚Äî]+)$', candidate):\n",
    "                    j += 1\n",
    "                    continue\n",
    "                next_para = candidate\n",
    "                break\n",
    "\n",
    "            if not next_para:\n",
    "                break  # nothing to merge\n",
    "\n",
    "            # Merge broken word + next paragraph\n",
    "            full_word = m.group(1) + next_para.split()[0]\n",
    "            rest = \" \".join(next_para.split()[1:])\n",
    "            cur = cur[:-(len(m.group(0)))] + full_word + \" \" + rest\n",
    "\n",
    "            if audit_writer:\n",
    "                audit_writer.writerow([\n",
    "                    filename or \"\",\n",
    "                    i,\n",
    "                    \"merge_broken_word_multi\",\n",
    "                    f\"{m.group(1)}- + {next_para[:30]}...\",\n",
    "                    cur[:50]+\"...\",\n",
    "                    cur[:50]+\"...\"\n",
    "                ])\n",
    "\n",
    "            # Move i forward to skip merged paragraphs\n",
    "            i = j\n",
    "        merged.append(cur)\n",
    "        i += 1\n",
    "    return merged\n",
    "\n",
    "def format_urdu_run(run):\n",
    "    \"\"\"\n",
    "    Apply Urdu formatting to a RUN only.\n",
    "    \"\"\"\n",
    "\n",
    "    rPr = run._element.get_or_add_rPr()\n",
    "\n",
    "    # RTL\n",
    "    rtl = OxmlElement(\"w:rtl\")\n",
    "    rPr.append(rtl)\n",
    "\n",
    "    # Language\n",
    "    lang = OxmlElement(\"w:lang\")\n",
    "    lang.set(qn(\"w:val\"), \"ur-PK\")\n",
    "    rPr.append(lang)\n",
    "\n",
    "    # Font\n",
    "    rFonts = OxmlElement(\"w:rFonts\")\n",
    "    rFonts.set(qn(\"w:ascii\"), \"Jameel Noori Nastaleeq\")\n",
    "    rFonts.set(qn(\"w:hAnsi\"), \"Jameel Noori Nastaleeq\")\n",
    "    rFonts.set(qn(\"w:cs\"), \"Jameel Noori Nastaleeq\")\n",
    "    rFonts.set(qn(\"w:fareast\"), \"Jameel Noori Nastaleeq\")\n",
    "    rPr.append(rFonts)\n",
    "\n",
    "    # Style\n",
    "    run.font.size = Pt(14)\n",
    "    run.bold = False\n",
    "\n",
    "def ocr_preclean(text):\n",
    "    \"\"\"Apply OCR-specific cleaning before paragraph-level normalization.\"\"\"\n",
    "    text = enhanced_ocr_preclean(text)\n",
    "    return text\n",
    "\n",
    "def is_heading(text):\n",
    "    return len(text.strip()) < 60\n",
    "\n",
    "# -----------------------\n",
    "# Full-document processing and I/O\n",
    "# -----------------------\n",
    "def process_docx_file(input_path, output_docx, audit_csv_path=None, audit=False, merge_paragraphs=False):\n",
    "    \"\"\"\n",
    "    Clean a single .docx file and write:\n",
    "      - cleaned .docx (output_docx)\n",
    "      - audit CSV listing automatic corrections (audit_csv_path)\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(input_path)\n",
    "\n",
    "    prev_was_heading = False\n",
    "\n",
    "    # read docx paragraphs\n",
    "    doc = docx.Document(str(input_path))\n",
    "    # Merge runs inside each paragraph\n",
    "    paras = [\"\".join(run.text for run in p.runs) for p in doc.paragraphs]\n",
    "\n",
    "    # prepare audit CSV writer\n",
    "    if audit:\n",
    "        audit_rows_file = open(audit_csv_path, \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "        audit_writer = csv.writer(audit_rows_file)\n",
    "        audit_writer.writerow(AUDIT_HEADER)\n",
    "    else:\n",
    "        class NullWriter:\n",
    "            def writerow(self, *args, **kwargs):\n",
    "                pass\n",
    "        audit_writer = NullWriter()\n",
    "        audit_rows_file = None\n",
    "        \n",
    "\n",
    "    # --- Detect repetitive headers automatically ---\n",
    "    detected_headers = detect_repetitive_headers(paras, min_repeats=3)\n",
    "    if detected_headers:\n",
    "        print(\"\\nüìò Detected possible repetitive headers:\")\n",
    "        for h in detected_headers:\n",
    "            print(\"   ‚Ä¢\", h)\n",
    "    # Clean each paragraph\n",
    "    cleaned_paras = []\n",
    "    for i, p in enumerate(paras):\n",
    "        if not p or p.isspace():\n",
    "            continue\n",
    "        cleaned = ocr_preclean(p)\n",
    "        if cleaned != p and audit_writer:\n",
    "            audit_writer.writerow([input_path.name, i, \"ocr_preclean\", \"\", _safe_preview(p), _safe_preview(cleaned)])\n",
    "        cleaned_paras.append(cleaned)\n",
    "        # Remove known repetitive headers (book title, Preface repeats, Appendix headers)\n",
    "        # --- Dynamically remove detected repetitive headers ---\n",
    "    auto_patterns = [re.escape(h) for h in detected_headers]\n",
    "    if auto_patterns:\n",
    "        dynamic_header_regex = [fr'^\\s*{p}\\s*$' for p in auto_patterns]\n",
    "        HEADER_PATTERNS.extend(dynamic_header_regex)\n",
    "        print(HEADER_PATTERNS)\n",
    "        \n",
    "    cleaned_paras = remove_known_headers(cleaned_paras, detected_headers, audit_writer=audit_writer, filename=input_path.name)\n",
    "\n",
    "    # Build new docx\n",
    "    new = docx.Document()\n",
    "    # --- Preserve original paragraph spacing\n",
    "    cleaned_text = \"\\n\\n\".join(cleaned_paras)\n",
    "    cleaned_text = merge_soft_split_paragraphs_text(cleaned_text)\n",
    "    cleaned_text = merge_paragraphs_split_by_hyphen(cleaned_text)\n",
    "    cleaned_paras = [p for p in cleaned_text.split(\"\\n\\n\") if p.strip()]\n",
    "    cleaned_paras = merge_broken_words(cleaned_paras, audit_writer=audit_writer, filename=input_path.name)\n",
    "\n",
    "    for para_text in cleaned_paras:\n",
    "        p = new.add_paragraph()\n",
    "\n",
    "        # split into words + spaces (keeps punctuation intact)\n",
    "        tokens = re.split(r\"(\\s+)\", para_text)\n",
    "\n",
    "        for tok in tokens:\n",
    "            run = p.add_run(tok)\n",
    "\n",
    "            if is_urdu_word(tok):\n",
    "                format_urdu_run(run)\n",
    "\n",
    "        current_is_heading = is_heading(para_text)\n",
    "\n",
    "        # Alignment logic\n",
    "        p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "        # Font logic\n",
    "        if current_is_heading:\n",
    "            for r in p.runs:\n",
    "                r.bold = True\n",
    "                r.font.size = Pt(14)\n",
    "\n",
    "            \n",
    "        # Update state\n",
    "        highlight_misspellings_in_paragraph(p)\n",
    "     \n",
    "    for sec in new.sections:\n",
    "        sec.top_margin = Inches(1)\n",
    "        sec.bottom_margin = Inches(1)\n",
    "        sec.left_margin = Inches(1.25)\n",
    "        sec.right_margin = Inches(1.25)\n",
    "    new.save(output_docx)\n",
    "    print(\"‚úÖ Cleaned DOCX:\", output_docx)\n",
    "\n",
    "    if audit_rows_file:\n",
    "        audit_rows_file.close()\n",
    "        print(\"‚úÖ Audit log:\", audit_csv_path)\n",
    "\n",
    "    return {\n",
    "        \"cleaned_docx\": str(output_docx)\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Batch helper\n",
    "# -----------------------\n",
    "def process_folder(root_dir, overwrite=False, audit=False):\n",
    "    root = Path(root_dir)\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(root)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for subdir in root.iterdir():\n",
    "        if not subdir.is_dir():\n",
    "            continue\n",
    "\n",
    "        docx_files = [\n",
    "            f for f in subdir.glob(\"*.docx\")\n",
    "            if not f.name.startswith(\"~$\")\n",
    "            and not f.stem.endswith(\".cleaned\")\n",
    "        ]\n",
    "\n",
    "        if not docx_files:\n",
    "            continue\n",
    "\n",
    "        for docx_file in docx_files:\n",
    "            output_docx = subdir / f\"{docx_file.stem}.cleaned.docx\"\n",
    "            audit_csv = subdir / f\"{docx_file.stem}.audit.csv\" if audit else None\n",
    "\n",
    "            if output_docx.exists() and not overwrite:\n",
    "                print(\"Skipping (exists):\", output_docx)\n",
    "                continue\n",
    "\n",
    "            print(f\"üìÇ Processing: {docx_file.name}\")\n",
    "\n",
    "            try:\n",
    "                res = process_docx_file(\n",
    "                    input_path=str(docx_file),\n",
    "                    output_docx=str(output_docx),\n",
    "                    audit_csv_path=str(audit_csv) if audit else None,\n",
    "                    audit=audit\n",
    "                )\n",
    "                results.append(res)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed: {docx_file} ‚Üí {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "\n",
    "process_docx_file(\"D:\\IPS assignments\\Assignment 5\\ŸÇÿ±ÿ¢ŸÜ ÿ≠⁄©€åŸÖ ÿßÿ±ÿ™ŸÇÿß€å ÿπŸÑ€å ÿ®ŸÜÿØ⁄Ø€å.docx\",\n",
    "                 \"cleaned2.docx\",\n",
    "                 audit_csv_path=\"none\",\n",
    "                 audit=False, merge_paragraphs=False)\n",
    "\n",
    "# process_folder(\"D:\\\\IPS assignments\\\\Assignment 9\", audit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f22238",
   "metadata": {},
   "source": [
    "\"D:\\IPS assignments\\Automation_program\\Project Folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c52fd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keyword matrix saved to: D:/IPS assignments/Assignment 8/keywords_yake.xlsx\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "import re\n",
    "\n",
    "def extract_keywords_yake(text, top_n=25):\n",
    "    if not text or len(text.strip()) < 100:\n",
    "        return []\n",
    "\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    extractor = yake.KeywordExtractor(\n",
    "        lan=\"en\",\n",
    "        n=3,\n",
    "        dedupLim=0.9,\n",
    "        top=top_n * 2\n",
    "    )\n",
    "\n",
    "    keywords = extractor.extract_keywords(text)\n",
    "\n",
    "    results = []\n",
    "    seen = set()\n",
    "\n",
    "    for kw, _ in keywords:\n",
    "        kw_norm = kw.lower().strip()\n",
    "        if len(kw_norm) < 3:\n",
    "            continue\n",
    "        if any(char.isdigit() for char in kw_norm):\n",
    "            continue\n",
    "        if kw_norm in seen:\n",
    "            continue\n",
    "\n",
    "        seen.add(kw_norm)\n",
    "        results.append(kw)\n",
    "\n",
    "        if len(results) == top_n:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import docx\n",
    "\n",
    "def collect_keywords_matrix_yake(root_dir, excel_path, top_n=15):\n",
    "    root = Path(root_dir)\n",
    "\n",
    "    keyword_map = {}  # document ‚Üí [keywords]\n",
    "\n",
    "    for subdir in root.iterdir():\n",
    "        if not subdir.is_dir():\n",
    "            continue\n",
    "\n",
    "        cleaned_docs = list(subdir.glob(\"*.cleaned.docx\"))\n",
    "        if not cleaned_docs:\n",
    "            continue\n",
    "\n",
    "        for docx_file in cleaned_docs:\n",
    "            doc = docx.Document(docx_file)\n",
    "            text = \"\\n\".join(\n",
    "                p.text for p in doc.paragraphs if p.text.strip()\n",
    "            )\n",
    "\n",
    "            keywords = extract_keywords_yake(text, top_n=top_n)\n",
    "            doc_name = docx_file.stem.replace(\".cleaned\", \"\")\n",
    "            keyword_map[doc_name] = keywords\n",
    "\n",
    "\n",
    "    if not keyword_map:\n",
    "        print(\"‚ö†Ô∏è No keywords extracted.\")\n",
    "        return\n",
    "\n",
    "    # Normalize column length\n",
    "    max_len = max(len(v) for v in keyword_map.values())\n",
    "\n",
    "    data = {\"keywords\": [\"\"] * max_len}\n",
    "\n",
    "    for doc, kws in keyword_map.items():\n",
    "        padded = kws + [\"\"] * (max_len - len(kws))\n",
    "        data[doc] = padded\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(excel_path, index=False)\n",
    "\n",
    "    print(\"‚úÖ Keyword matrix saved to:\", excel_path)\n",
    "\n",
    "\n",
    "\n",
    "collect_keywords_matrix_yake(\n",
    "    root_dir=\"D:/IPS assignments/Assignment 8\",\n",
    "    excel_path=\"D:/IPS assignments/Assignment 8/keywords_yake.xlsx\", top_n=20\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97875a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ post-budget economy- issues and challenges.cleaned.docx\n",
      "‚úÖ Keyword extraction complete: D:/IPS assignments/Assignment 8/keywords_KeyBERT_fenhanced.xlsx\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import re\n",
    "from pathlib import Path\n",
    "import docx\n",
    "import pandas as pd\n",
    "from zipfile import BadZipFile\n",
    "from collections import Counter\n",
    "import hashlib\n",
    "import spacy\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load models ONCE\n",
    "# --------------------------------------------------\n",
    "kw_model = KeyBERT(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Normalization (STRICT)\n",
    "# --------------------------------------------------\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Structural DOCX reader (weighted)\n",
    "# --------------------------------------------------\n",
    "def read_docx_weighted(path: Path) -> str:\n",
    "    doc = docx.Document(path)\n",
    "    chunks = []\n",
    "\n",
    "    for p in doc.paragraphs:\n",
    "        txt = p.text.strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "\n",
    "        style = p.style.name.lower()\n",
    "\n",
    "        if style.startswith(\"heading\"):\n",
    "            chunks.extend([txt] * 3)\n",
    "        elif txt.isupper():\n",
    "            chunks.extend([txt] * 2)\n",
    "        else:\n",
    "            chunks.append(txt)\n",
    "\n",
    "    return normalize_text(\" \".join(chunks))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Fingerprint (duplicate detection)\n",
    "# --------------------------------------------------\n",
    "def fingerprint(text: str) -> str:\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Adaptive keyword count\n",
    "# --------------------------------------------------\n",
    "def adaptive_top_n(word_count: int) -> int:\n",
    "    if word_count < 500:\n",
    "        return 8\n",
    "    elif word_count < 1500:\n",
    "        return 12\n",
    "    return 18\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. Phrase control rules (CRITICAL)\n",
    "# --------------------------------------------------\n",
    "BAD_ENDINGS = {\n",
    "    \"current\", \"selective\", \"general\", \"various\",\n",
    "    \"major\", \"minor\", \"differences\", \"notwithstanding\"\n",
    "}\n",
    "\n",
    "BANNED_SINGLE_WORDS = {\n",
    "    \"year\", \"focus\", \"change\", \"error\", \"demands\",\n",
    "    \"comparability\", \"indicators\", \"reduction\",\n",
    "    \"growth\", \"finance\", \"agriculture\", \"institutions\",\n",
    "    \"pakistan\", \"prosperity\"\n",
    "}\n",
    "\n",
    "WHITELIST_SINGLE_WORDS = {\n",
    "    \"poverty\", \"inflation\", \"democracy\"\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. Extract noun-phrase candidates ONLY\n",
    "# --------------------------------------------------\n",
    "def extract_noun_phrases(text: str):\n",
    "    doc = nlp(text)\n",
    "    phrases = set()\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        phrase = chunk.text.lower().strip()\n",
    "\n",
    "        if len(phrase) < 4:\n",
    "            continue\n",
    "        if any(char.isdigit() for char in phrase):\n",
    "            continue\n",
    "\n",
    "        tokens = phrase.split()\n",
    "\n",
    "        # üö´ Ban meaningless single words\n",
    "        if len(tokens) == 1:\n",
    "            if phrase not in WHITELIST_SINGLE_WORDS:\n",
    "                continue\n",
    "\n",
    "        if phrase in BANNED_SINGLE_WORDS:\n",
    "            continue\n",
    "\n",
    "        if tokens[-1] in BAD_ENDINGS:\n",
    "            continue\n",
    "\n",
    "        phrases.add(phrase)\n",
    "\n",
    "    return list(phrases)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 8. Keyword extraction (STRICT KeyBERT)\n",
    "# --------------------------------------------------\n",
    "def extract_keywords_keybert(text: str):\n",
    "    words = text.split()\n",
    "    if len(words) < 200:\n",
    "        return []\n",
    "\n",
    "    candidates = extract_noun_phrases(text)\n",
    "    if len(candidates) < 10:\n",
    "        return []\n",
    "\n",
    "    top_n = adaptive_top_n(len(words))\n",
    "\n",
    "    raw = kw_model.extract_keywords(\n",
    "        text,\n",
    "        candidates=candidates,\n",
    "        use_mmr=True,\n",
    "        diversity=0.45,\n",
    "        top_n=top_n\n",
    "    )\n",
    "\n",
    "    cleaned = []\n",
    "    for kw, _ in raw:\n",
    "        kw = kw.strip()\n",
    "\n",
    "        tokens = kw.split()\n",
    "\n",
    "        # Final enforcement: prefer multi-word concepts\n",
    "        if len(tokens) < 2 and kw not in WHITELIST_SINGLE_WORDS:\n",
    "            continue\n",
    "\n",
    "        cleaned.append(kw)\n",
    "\n",
    "        if len(cleaned) == top_n:\n",
    "            break\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 9. Cross-document IDF reweighting\n",
    "# --------------------------------------------------\n",
    "def apply_corpus_idf(keyword_map: dict):\n",
    "    df_counter = Counter()\n",
    "    total_docs = len(keyword_map)\n",
    "\n",
    "    for kws in keyword_map.values():\n",
    "        for kw in set(kws):\n",
    "            df_counter[kw] += 1\n",
    "\n",
    "    idf = {kw: total_docs / df_counter[kw] for kw in df_counter}\n",
    "\n",
    "    reranked = {\n",
    "        doc: sorted(kws, key=lambda k: -idf.get(k, 0))\n",
    "        for doc, kws in keyword_map.items()\n",
    "    }\n",
    "\n",
    "    idf_table = pd.DataFrame(\n",
    "        [(kw, df_counter[kw], idf[kw]) for kw in df_counter],\n",
    "        columns=[\"keyword\", \"doc_frequency\", \"idf_score\"]\n",
    "    ).sort_values(\"idf_score\", ascending=False)\n",
    "\n",
    "    return reranked, idf_table\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 10. Corpus processor\n",
    "# --------------------------------------------------\n",
    "def collect_keywords_matrix_keybert(root_dir: str, excel_path: str):\n",
    "    root = Path(root_dir)\n",
    "    keyword_map = {}\n",
    "    seen_fingerprints = set()\n",
    "\n",
    "    for docx_file in root.rglob(\"*.cleaned.docx\"):\n",
    "        print(f\"üîÑ Processing: {docx_file.name}\")\n",
    "\n",
    "        try:\n",
    "            text = read_docx_weighted(docx_file)\n",
    "        except BadZipFile:\n",
    "            print(f\"‚ö†Ô∏è Skipping invalid DOCX: {docx_file}\")\n",
    "            continue\n",
    "\n",
    "        fp = fingerprint(text)\n",
    "        if fp in seen_fingerprints:\n",
    "            print(f\"‚ö†Ô∏è Duplicate skipped: {docx_file.name}\")\n",
    "            continue\n",
    "        seen_fingerprints.add(fp)\n",
    "\n",
    "        keywords = extract_keywords_keybert(text)\n",
    "        if not keywords:\n",
    "            continue\n",
    "\n",
    "        doc_name = docx_file.stem.replace(\".cleaned\", \"\")\n",
    "        keyword_map[doc_name] = keywords\n",
    "\n",
    "    if not keyword_map:\n",
    "        print(\"‚ö†Ô∏è No keywords extracted.\")\n",
    "        return\n",
    "\n",
    "    keyword_map, idf_table = apply_corpus_idf(keyword_map)\n",
    "\n",
    "    max_len = max(len(v) for v in keyword_map.values())\n",
    "    data = {\n",
    "        doc: kws + [\"\"] * (max_len - len(kws))\n",
    "        for doc, kws in keyword_map.items()\n",
    "    }\n",
    "\n",
    "    df_keywords = pd.DataFrame(data)\n",
    "    df_keywords.index.name = \"Rank\"\n",
    "\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "        df_keywords.to_excel(writer, sheet_name=\"Keywords\", index=True)\n",
    "        idf_table.to_excel(writer, sheet_name=\"Corpus_IDF\", index=False)\n",
    "\n",
    "    print(\"‚úÖ FINAL strict keyword matrix saved to:\", excel_path)\n",
    "\n",
    "# -----------------------------------\n",
    "# 10. Run\n",
    "# -----------------------------------\n",
    "collect_keywords_matrix_keybert(\n",
    "    root_dir=\"D:\\\\IPS assignments\\\\Assignment 8\\\\5-106 Post Budget Economy- Issues and Challenges\",\n",
    "    excel_path=\"D:/IPS assignments/Assignment 8/keywords_KeyBERT_enhanced.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32ac1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All .cleaned files collected and renamed successfully.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def collect_and_rename_cleaned_files(\n",
    "    source_root: str,\n",
    "    output_dir: str\n",
    "):\n",
    "    source_root = Path(source_root)\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_path in source_root.rglob(\"*\"):\n",
    "        if file_path.is_file() and \".cleaned\" in file_path.stem:\n",
    "            # Remove \".cleaned\" from filename\n",
    "            new_name = file_path.name.replace(\".cleaned\", \"\")\n",
    "            target_path = output_dir / new_name\n",
    "\n",
    "            # Handle name collisions\n",
    "            counter = 1\n",
    "            while target_path.exists():\n",
    "                target_path = output_dir / f\"{file_path.stem.replace('.cleaned','')}_{counter}{file_path.suffix}\"\n",
    "                counter += 1\n",
    "\n",
    "            shutil.copy2(file_path, target_path)\n",
    "\n",
    "    print(\"‚úÖ All .cleaned files collected and renamed successfully.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# USAGE\n",
    "# =========================\n",
    "collect_and_rename_cleaned_files(\n",
    "    source_root=r\"D:\\IPS assignments\\Assignment 8\",\n",
    "    output_dir=r\"D:\\IPS assignments\\cleaned_files\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
